apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: retrain
  annotations:
    tekton.dev/output_artifacts: '{"run-a-file": [{"key": "artifacts/$PIPELINERUN/run-a-file/mlpipeline-metrics.tgz",
      "name": "mlpipeline-metrics", "path": "/tmp/mlpipeline-metrics.json"}, {"key":
      "artifacts/$PIPELINERUN/run-a-file/mlpipeline-ui-metadata.tgz", "name": "mlpipeline-ui-metadata",
      "path": "/tmp/mlpipeline-ui-metadata.json"}], "run-a-file-2": [{"key": "artifacts/$PIPELINERUN/run-a-file-2/mlpipeline-metrics.tgz",
      "name": "mlpipeline-metrics", "path": "/tmp/mlpipeline-metrics.json"}, {"key":
      "artifacts/$PIPELINERUN/run-a-file-2/mlpipeline-ui-metadata.tgz", "name": "mlpipeline-ui-metadata",
      "path": "/tmp/mlpipeline-ui-metadata.json"}], "run-a-file-3": [{"key": "artifacts/$PIPELINERUN/run-a-file-3/mlpipeline-metrics.tgz",
      "name": "mlpipeline-metrics", "path": "/tmp/mlpipeline-metrics.json"}, {"key":
      "artifacts/$PIPELINERUN/run-a-file-3/mlpipeline-ui-metadata.tgz", "name": "mlpipeline-ui-metadata",
      "path": "/tmp/mlpipeline-ui-metadata.json"}]}'
    tekton.dev/input_artifacts: '{}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"run-a-file": [["mlpipeline-metrics", "/tmp/mlpipeline-metrics.json"],
      ["mlpipeline-ui-metadata", "/tmp/mlpipeline-ui-metadata.json"]], "run-a-file-2":
      [["mlpipeline-metrics", "/tmp/mlpipeline-metrics.json"], ["mlpipeline-ui-metadata",
      "/tmp/mlpipeline-ui-metadata.json"]], "run-a-file-3": [["mlpipeline-metrics",
      "/tmp/mlpipeline-metrics.json"], ["mlpipeline-ui-metadata", "/tmp/mlpipeline-ui-metadata.json"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "https://minio-api-ai-demo.apps.cluster-rhhz8.dynamic.redhatworkshops.io",
      "name": "s3endpoint", "optional": true, "type": "String"}, {"default": "data",
      "name": "s3bucket_data", "optional": true, "type": "String"}, {"default": "production",
      "name": "s3bucket_models", "optional": true, "type": "String"}, {"default":
      "/data/", "name": "mount_path", "optional": true, "type": "String"}], "name":
      "retrain"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  params:
  - name: mount_path
    value: /data/
  - name: s3bucket_data
    value: data
  - name: s3bucket_models
    value: production
  - name: s3endpoint
    value: https://minio-api-ai-demo.apps.cluster-rhhz8.dynamic.redhatworkshops.io
  pipelineSpec:
    params:
    - name: mount_path
      default: /data/
    - name: s3bucket_data
      default: data
    - name: s3bucket_models
      default: production
    - name: s3endpoint
      default: https://minio-api-ai-demo.apps.cluster-rhhz8.dynamic.redhatworkshops.io
    tasks:
    - name: run-a-file
      params:
      - name: mount_path
        value: $(params.mount_path)
      - name: s3bucket_data
        value: $(params.s3bucket_data)
      - name: s3endpoint
        value: $(params.s3endpoint)
      taskSpec:
        steps:
        - name: main
          args:
          - |
            s3endpoint="$0"
            s3bucket_data="$1"
            mount_path="$2"
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'retrain' --cos-endpoint 'https://minio-api-test03.apps.rhods-internal.61tk.p1.openshiftapps.com' --cos-bucket 'tb1' --cos-directory 'retrain-1219103719' --cos-dependencies-archive 'step-01-6764011e-55f5-4d4d-a8c2-252b1de09bc8.tar.gz' --file 'rhods-transfer-learning/workbench/pipeline/step-01.ipynb' --pipeline-parameters 's3endpoint=$s3endpoint;s3bucket_data=$s3bucket_data;mount_path=$mount_path' --parameter-pass-method 'env' "
          - $(inputs.params.s3endpoint)
          - $(inputs.params.s3bucket_data)
          - $(inputs.params.mount_path)
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-minio3
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-minio3
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          image: quay.io/opendatahub-contrib/workbench-images:cuda-runtime-tensorflow-c9s-py39_2023c_latest
          volumeMounts:
          - mountPath: /data
            name: pipeline-pvc
            readOnly: false
        params:
        - name: mount_path
        - name: s3bucket_data
        - name: s3endpoint
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        - name: pipeline-pvc
          persistentVolumeClaim:
            claimName: pipeline-pvc
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: retrain
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: load_data
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: rhods-transfer-learning/workbench/pipeline/step-01.ipynb
            elyra/pipeline-source: retrain.pipeline
            pipelines.kubeflow.org/task_display_name: load_data
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=75f955f3b23646f6be2103a9beed5238fd6b63756add33e1a0264ff919fc2437"}'
    - name: run-a-file-2
      params:
      - name: mount_path
        value: $(params.mount_path)
      taskSpec:
        steps:
        - name: main
          args:
          - |
            mount_path="$0"
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'retrain' --cos-endpoint 'https://minio-api-test03.apps.rhods-internal.61tk.p1.openshiftapps.com' --cos-bucket 'tb1' --cos-directory 'retrain-1219103719' --cos-dependencies-archive 'step-02-8af3247b-bc48-42f1-9f25-95e1de81169a.tar.gz' --file 'rhods-transfer-learning/workbench/pipeline/step-02.ipynb' --pipeline-parameters 'mount_path=$mount_path' --parameter-pass-method 'env' "
          - $(inputs.params.mount_path)
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-minio3
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-minio3
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          image: quay.io/opendatahub-contrib/workbench-images:cuda-runtime-tensorflow-c9s-py39_2023c_latest
          volumeMounts:
          - mountPath: /data
            name: pipeline-pvc
            readOnly: false
        params:
        - name: mount_path
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        - name: pipeline-pvc
          persistentVolumeClaim:
            claimName: pipeline-pvc
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: retrain
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: create_model
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: rhods-transfer-learning/workbench/pipeline/step-02.ipynb
            elyra/pipeline-source: retrain.pipeline
            pipelines.kubeflow.org/task_display_name: create_model
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=8c19c0ee548a2a9ea20c1b5f4e723314e92dd23c05d8bbd0d6685b4161512b83"}'
      runAfter:
      - run-a-file
    - name: run-a-file-3
      params:
      - name: mount_path
        value: $(params.mount_path)
      - name: s3bucket_models
        value: $(params.s3bucket_models)
      - name: s3endpoint
        value: $(params.s3endpoint)
      taskSpec:
        steps:
        - name: main
          args:
          - |
            s3endpoint="$0"
            s3bucket_models="$1"
            mount_path="$2"
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'retrain' --cos-endpoint 'https://minio-api-test03.apps.rhods-internal.61tk.p1.openshiftapps.com' --cos-bucket 'tb1' --cos-directory 'retrain-1219103719' --cos-dependencies-archive 'step-03-9860d935-5991-4391-a6dd-2e0af6f912a3.tar.gz' --file 'rhods-transfer-learning/workbench/pipeline/step-03.ipynb' --pipeline-parameters 's3endpoint=$s3endpoint;s3bucket_models=$s3bucket_models;mount_path=$mount_path' --parameter-pass-method 'env' "
          - $(inputs.params.s3endpoint)
          - $(inputs.params.s3bucket_models)
          - $(inputs.params.mount_path)
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-minio3
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-minio3
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          image: quay.io/opendatahub-contrib/workbench-images:cuda-runtime-tensorflow-c9s-py39_2023c_latest
          volumeMounts:
          - mountPath: /data
            name: pipeline-pvc
            readOnly: false
        params:
        - name: mount_path
        - name: s3bucket_models
        - name: s3endpoint
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        - name: pipeline-pvc
          persistentVolumeClaim:
            claimName: pipeline-pvc
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: retrain
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: push_model
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: rhods-transfer-learning/workbench/pipeline/step-03.ipynb
            elyra/pipeline-source: retrain.pipeline
            pipelines.kubeflow.org/task_display_name: push_model
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=fa136848e6e431e158051e6e5eee2405d144e53d68e10604c1a00d60f8b02d14"}'
      runAfter:
      - run-a-file-2
